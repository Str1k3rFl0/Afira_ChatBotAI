{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1abfe239-7e53-442a-a299-562b36123f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import math\n",
    "import pickle\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1d41f6-005d-484a-b218-474b924b9d75",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1630982-c020-4611-925b-8bfc78f85920",
   "metadata": {},
   "source": [
    "## Load JSON file & show details about the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82837335-b7b6-4850-8836-59533a668998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading data...\")\n",
    "with open('chatbotdata.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62207db1-5cab-4a08-ad46-8ad24091c129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Loaded 1159 patterns from 20 intents\n",
      "\n",
      "\n",
      " Intent distribution:\n",
      "    goodbye: 107 patterns\n",
      "    thanks: 105 patterns\n",
      "    greeting: 104 patterns\n",
      "    about_owner: 74 patterns\n",
      "    price_website: 67 patterns\n",
      "    pred_types: 67 patterns\n",
      "    price_ml: 64 patterns\n",
      "    predictions: 63 patterns\n",
      "    about_model: 59 patterns\n",
      "    ask_weather: 54 patterns\n",
      "    capabilities: 52 patterns\n",
      "    who_are_you: 51 patterns\n",
      "    motivation: 48 patterns\n",
      "    contact: 47 patterns\n",
      "    education: 40 patterns\n",
      "    projects: 36 patterns\n",
      "    compliment: 34 patterns\n",
      "    skills: 33 patterns\n",
      "    ask_time: 30 patterns\n",
      "    joke: 24 patterns\n"
     ]
    }
   ],
   "source": [
    "patterns = []\n",
    "labels = []\n",
    "intent_counts = {}\n",
    "\n",
    "for intent in data['intents']:\n",
    "    intent_name = intent['name']\n",
    "    intent_counts[intent_name] = len(intent['patterns'])\n",
    "\n",
    "    for pattern in intent['patterns']:\n",
    "        if pattern.strip():\n",
    "            patterns.append(pattern)\n",
    "            labels.append(intent_name)\n",
    "\n",
    "print(f\"\\n Loaded {len(patterns)} patterns from {len(data['intents'])} intents\")\n",
    "print(\"\\n\\n Intent distribution:\")\n",
    "for intent_name, count in sorted(intent_counts.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"    {intent_name}: {count} patterns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b522d4b-bfb4-4c60-a1af-41b748296bf8",
   "metadata": {},
   "source": [
    "# Apply TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54fde1f5-d813-4665-bc80-cc1b44d8c351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 754\n"
     ]
    }
   ],
   "source": [
    "# Simple Tokenizer\n",
    "def tokenize(text):\n",
    "    return text.lower().split()\n",
    "\n",
    "vocab = set()\n",
    "for pattern in patterns:\n",
    "    vocab.update(tokenize(pattern))\n",
    "\n",
    "vocab = sorted(vocab)\n",
    "word2idx = {word: i for i, word in enumerate(vocab)}\n",
    "print(f\"Vocabulary size: {len(vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "814acc12-a06d-4979-b0b9-3034921f7ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculate TF (Term Frequency) for each document\n",
    "def compute_tf(document):\n",
    "    tokens = tokenize(document)\n",
    "    tf_counter = Counter(tokens)\n",
    "    total_count = len(tokens)\n",
    "    tf_vector = np.zeros(len(vocab))\n",
    "\n",
    "    for token, count in tf_counter.items():\n",
    "        if token in word2idx:\n",
    "            tf_vector[word2idx[token]] = count / total_count\n",
    "    return tf_vector\n",
    "\n",
    "tf_vectors = np.array([compute_tf(p) for p in patterns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "422a5ddb-566f-4714-bf38-487c52598307",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculate IDF (Inverse Document Freq. )\n",
    "N = len(patterns)\n",
    "df = np.zeros(len(vocab))\n",
    "\n",
    "for i, word in enumerate(vocab):\n",
    "    df[i] = sum(1 for pattern in patterns if word in tokenize(pattern))\n",
    "\n",
    "idf = np.log((1 + N) / (1 + df)) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9a4cd3f-eeda-480f-b86a-4b445a1843d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF matrix shape: (1159, 754)\n"
     ]
    }
   ],
   "source": [
    "tfidf_vectors = tf_vectors * idf\n",
    "print(\"TF-IDF matrix shape:\", tfidf_vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44304e5c-d16f-4e6f-9794-0b06309ddc14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First pattern: Hello\n",
      "Vector TF-IDF: [0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         5.41711795 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "### Verify an TF-IDF vector\n",
    "print(\"First pattern:\", patterns[0])\n",
    "print(\"Vector TF-IDF:\", tfidf_vectors[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "74934ea8-871e-4bbf-a494-22dec8ee5913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'â€”', 'generation.', 'gator', 'game?']\n"
     ]
    }
   ],
   "source": [
    "## Find most important words for each document\n",
    "## Let's say.. we want for the first document\n",
    "top_idx = tfidf_vectors[0].argsort()[::-1][:5]\n",
    "print([vocab[i] for i in top_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee062a2f-6043-48c0-8b2f-9b71fb683bbf",
   "metadata": {},
   "source": [
    "# Train the model using LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "737c249b-ed5f-4804-a596-54df26641803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num. classes: ['about_model' 'about_owner' 'ask_time' 'ask_weather' 'capabilities'\n",
      " 'compliment' 'contact' 'education' 'goodbye' 'greeting' 'joke'\n",
      " 'motivation' 'pred_types' 'predictions' 'price_ml' 'price_website'\n",
      " 'projects' 'skills' 'thanks' 'who_are_you']\n"
     ]
    }
   ],
   "source": [
    "## Apply Label Encoding\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(labels)\n",
    "\n",
    "pickle.dump(le, open('label_encoder.pkl', 'wb'))\n",
    "print(f\"Num. classes: {le.classes_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "342cd231-d081-406e-9af6-9ad253881c25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (927, 754), y_train shape: (927,)\n",
      "X_test shape: (232, 754), y_test shape: (232,)\n"
     ]
    }
   ],
   "source": [
    "## Split the data ( train/test split )\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    tfidf_vectors,\n",
    "    y_encoded,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y_encoded)\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9e61bfdf-9fdb-4503-b389-925702e39944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR model was trained and saved on (nlp_model_lr.pkl).\n"
     ]
    }
   ],
   "source": [
    "## Train the model using Logistic Regression\n",
    "model = LogisticRegression(max_iter=500, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "pickle.dump(model, open('nlp_model_lr.pkl', 'wb'))\n",
    "print(\"LR model was trained and saved on (nlp_model_lr.pkl).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "91c72518-2031-4a82-9a81-f08bbff232a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model acc. on data test set: 0.8836\n",
      "\n",
      "\n",
      "Detailed clasification:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "  about_model       0.71      1.00      0.83        12\n",
      "  about_owner       0.94      1.00      0.97        15\n",
      "     ask_time       1.00      1.00      1.00         6\n",
      "  ask_weather       1.00      0.91      0.95        11\n",
      " capabilities       0.57      0.40      0.47        10\n",
      "   compliment       1.00      0.71      0.83         7\n",
      "      contact       1.00      1.00      1.00         9\n",
      "    education       1.00      0.62      0.77         8\n",
      "      goodbye       0.94      0.81      0.87        21\n",
      "     greeting       1.00      0.95      0.98        21\n",
      "         joke       1.00      1.00      1.00         5\n",
      "   motivation       0.91      1.00      0.95        10\n",
      "   pred_types       0.87      1.00      0.93        13\n",
      "  predictions       0.92      0.92      0.92        13\n",
      "     price_ml       0.63      0.92      0.75        13\n",
      "price_website       0.80      0.92      0.86        13\n",
      "     projects       1.00      1.00      1.00         7\n",
      "       skills       1.00      1.00      1.00         7\n",
      "       thanks       1.00      0.95      0.98        21\n",
      "  who_are_you       0.57      0.40      0.47        10\n",
      "\n",
      "     accuracy                           0.88       232\n",
      "    macro avg       0.89      0.88      0.88       232\n",
      " weighted avg       0.89      0.88      0.88       232\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Evaluate the model\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Model acc. on data test set: {accuracy:.4f}\")\n",
    "\n",
    "print(\"\\n\\nDetailed clasification:\")\n",
    "print(classification_report(y_test, y_pred, target_names=le.classes_, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7f4c1bf6-6561-4b9d-9bdd-40576ead7177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary and IDF saved!\n"
     ]
    }
   ],
   "source": [
    "pickle.dump(vocab, open('vocab.pkl', 'wb'))\n",
    "pickle.dump(word2idx, open('word2idx.pkl', 'wb'))\n",
    "pickle.dump(idf, open('idf.pkl', 'wb'))\n",
    "print(\"Vocabulary and IDF saved!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
