{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1abfe239-7e53-442a-a299-562b36123f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import math\n",
    "import pickle\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1d41f6-005d-484a-b218-474b924b9d75",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1630982-c020-4611-925b-8bfc78f85920",
   "metadata": {},
   "source": [
    "## Load JSON file & show details about the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82837335-b7b6-4850-8836-59533a668998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading data...\")\n",
    "with open('chatbotdata.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62207db1-5cab-4a08-ad46-8ad24091c129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Loaded 6413 patterns from 11 intents\n",
      "\n",
      "\n",
      " Intent distribution:\n",
      "    thanks: 2826 patterns\n",
      "    goodbye: 2249 patterns\n",
      "    greeting: 987 patterns\n",
      "    about_owner: 74 patterns\n",
      "    capabilities: 52 patterns\n",
      "    who_are_you: 51 patterns\n",
      "    contact: 47 patterns\n",
      "    projects: 36 patterns\n",
      "    compliment: 34 patterns\n",
      "    skills: 33 patterns\n",
      "    joke: 24 patterns\n"
     ]
    }
   ],
   "source": [
    "patterns = []\n",
    "labels = []\n",
    "intent_counts = {}\n",
    "\n",
    "for intent in data['intents']:\n",
    "    intent_name = intent['name']\n",
    "    intent_counts[intent_name] = len(intent['patterns'])\n",
    "\n",
    "    for pattern in intent['patterns']:\n",
    "        if pattern.strip():\n",
    "            patterns.append(pattern)\n",
    "            labels.append(intent_name)\n",
    "\n",
    "print(f\"\\n Loaded {len(patterns)} patterns from {len(data['intents'])} intents\")\n",
    "print(\"\\n\\n Intent distribution:\")\n",
    "for intent_name, count in sorted(intent_counts.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"    {intent_name}: {count} patterns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cda8906-0e08-4e99-ac4e-9eff36e6e965",
   "metadata": {},
   "source": [
    "### Because we have some intents with < 1% patterns"
   ]
  },
  {
   "cell_type": "raw",
   "id": "08b3f311-b7fc-4cd5-a297-5eb338888ac3",
   "metadata": {},
   "source": [
    "There's overfitting on major intentsÂ¶\n",
    "Imbalance in training - weak performance for \"skills\", \"joke\" etc etc..\n",
    "So... We use TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b522d4b-bfb4-4c60-a1af-41b748296bf8",
   "metadata": {},
   "source": [
    "# Apply TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54fde1f5-d813-4665-bc80-cc1b44d8c351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 428\n"
     ]
    }
   ],
   "source": [
    "# Simple Tokenizer\n",
    "def tokenize(text):\n",
    "    return text.lower().split()\n",
    "\n",
    "vocab = set()\n",
    "for pattern in patterns:\n",
    "    vocab.update(tokenize(pattern))\n",
    "\n",
    "vocab = sorted(vocab)\n",
    "word2idx = {word: i for i, word in enumerate(vocab)}\n",
    "print(f\"Vocabulary size: {len(vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "814acc12-a06d-4979-b0b9-3034921f7ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculate TF (Term Frequency) for each document\n",
    "def compute_tf(document):\n",
    "    tokens = tokenize(document)\n",
    "    tf_counter = Counter(tokens)\n",
    "    total_count = len(tokens)\n",
    "    tf_vector = np.zeros(len(vocab))\n",
    "\n",
    "    for token, count in tf_counter.items():\n",
    "        if token in word2idx:\n",
    "            tf_vector[word2idx[token]] = count / total_count\n",
    "    return tf_vector\n",
    "\n",
    "tf_vectors = np.array([compute_tf(p) for p in patterns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "422a5ddb-566f-4714-bf38-487c52598307",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculate IDF (Inverse Document Freq. )\n",
    "N = len(patterns)\n",
    "df = np.zeros(len(vocab))\n",
    "\n",
    "for i, word in enumerate(vocab):\n",
    "    df[i] = sum(1 for pattern in patterns if word in tokenize(pattern))\n",
    "\n",
    "idf = np.log((1 + N) / (1 + df)) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9a4cd3f-eeda-480f-b86a-4b445a1843d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF matrix shape: (6413, 428)\n"
     ]
    }
   ],
   "source": [
    "tfidf_vectors = tf_vectors * idf\n",
    "print(\"TF-IDF matrix shape:\", tfidf_vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "44304e5c-d16f-4e6f-9794-0b06309ddc14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First pattern: nice to see you dude\n",
      "Vector TF-IDF: [0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.9812852  0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         1.22573044 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.74235981 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.97668729 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.54831787 0.         0.\n",
      " 0.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "### Verify an TF-IDF vector\n",
    "print(\"First pattern:\", patterns[0])\n",
    "print(\"Vector TF-IDF:\", tfidf_vectors[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "74934ea8-871e-4bbf-a494-22dec8ee5913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nice', 'dude', 'to', 'see', 'you']\n"
     ]
    }
   ],
   "source": [
    "## Find most important words for each document\n",
    "## Let's say.. we want for the first document\n",
    "top_idx = tfidf_vectors[0].argsort()[::-1][:5]\n",
    "print([vocab[i] for i in top_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee062a2f-6043-48c0-8b2f-9b71fb683bbf",
   "metadata": {},
   "source": [
    "# Train the model using LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "737c249b-ed5f-4804-a596-54df26641803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num. classes: ['about_owner' 'capabilities' 'compliment' 'contact' 'goodbye' 'greeting'\n",
      " 'joke' 'projects' 'skills' 'thanks' 'who_are_you']\n"
     ]
    }
   ],
   "source": [
    "## Apply Label Encoding\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(labels)\n",
    "\n",
    "pickle.dump(le, open('label_encoder.pkl', 'wb'))\n",
    "print(f\"Num. classes: {le.classes_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "342cd231-d081-406e-9af6-9ad253881c25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (5130, 428), y_train shape: (5130,)\n",
      "X_test shape: (1283, 428), y_test shape: (1283,)\n"
     ]
    }
   ],
   "source": [
    "## Split the data ( train/test split )\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    tfidf_vectors,\n",
    "    y_encoded,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y_encoded)\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9e61bfdf-9fdb-4503-b389-925702e39944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR model was trained and saved on (nlp_model_lr.pkl).\n"
     ]
    }
   ],
   "source": [
    "## Train the model using Logistic Regression\n",
    "model = LogisticRegression(max_iter=500, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "pickle.dump(model, open('nlp_model_lr.pkl', 'wb'))\n",
    "print(\"LR model was trained and saved on (nlp_model_lr.pkl).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "91c72518-2031-4a82-9a81-f08bbff232a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model acc. on data test set: 0.9914\n",
      "\n",
      "\n",
      "Detailed clasification:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " about_owner       1.00      0.87      0.93        15\n",
      "capabilities       1.00      0.60      0.75        10\n",
      "  compliment       1.00      1.00      1.00         7\n",
      "     contact       1.00      0.89      0.94         9\n",
      "     goodbye       1.00      1.00      1.00       450\n",
      "    greeting       1.00      1.00      1.00       198\n",
      "        joke       1.00      0.80      0.89         5\n",
      "    projects       0.75      0.86      0.80         7\n",
      "      skills       0.83      0.71      0.77         7\n",
      "      thanks       1.00      1.00      1.00       565\n",
      " who_are_you       0.71      1.00      0.83        10\n",
      "\n",
      "    accuracy                           0.99      1283\n",
      "   macro avg       0.94      0.88      0.90      1283\n",
      "weighted avg       0.99      0.99      0.99      1283\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Evaluate the model\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Model acc. on data test set: {accuracy:.4f}\")\n",
    "\n",
    "print(\"\\n\\nDetailed clasification:\")\n",
    "print(classification_report(y_test, y_pred, target_names=le.classes_, zero_division=0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
